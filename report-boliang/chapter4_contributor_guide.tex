\chapter{Contributor Guide}
\label{ch:contributor_guide}

\section{Why a Contributor Guide is Necessary}
\label{sec:why_guide}
A contributor guide is a critical document for any open-source software project, particularly in a scientific context like Eelbrain.
Its primary purpose is to standardize the development process and lower the barrier to entry for new contributors.
For a project to thrive and grow, it must be able to attract and effectively integrate new developers, who may range from students to researchers at other institutions. 

A well-written guide provides a clear roadmap for everything from setting up a development environment and running tests to following coding conventions and submitting code for review.
This standardization ensures that all contributions maintain a consistent level of quality, making the codebase more coherent and easier to manage in the long term.
By removing ambiguity and providing clear instructions, a contributor guide empowers potential collaborators, reduces the burden on core developers to answer repetitive questions, and fosters a welcoming and productive community.

\section{Recommendations: A Development Guide for Future Contributors}
\label{sec:recommendations_guide}
To address these challenges and support the future growth of Eelbrain, creating a comprehensive contributor guide is a crucial next step.
This guide should be a living document, accessible in the project's main repository, and should cover several key areas:

\subsection{Coding Conventions}
To maintain a consistent and readable codebase, we adhere to the following conventions, which were established and refined through code review discussions.

\begin{itemize}
    \item \textbf{Style Guide:} All Python code must adhere to the \href{https://peps.python.org/pep-0008/}{PEP 8 style guide}.
    We use \texttt{flake8} to check for compliance. Before submitting your code, please run \texttt{flake8} locally and fix any reported issues.
    You can use tools like \texttt{autopep8} to automatically fix many common style issues.

    \item \textbf{Import Order:} Imports should be grouped in the following order, with a blank line separating each group:
    \begin{enumerate}
        \item Standard library imports (e.g., \texttt{io}, \texttt{os}).
        \item Related third-party imports (e.g., \texttt{numpy}, \texttt{matplotlib}).
        \item Local application/library specific imports (e.g., \texttt{from eelbrain import ...}).
    \end{enumerate}
    \begin{quote}
        \textit{Case Study:} In one review, a contributor was reminded to reorder their imports to place standard library modules before third-party and local imports.
        Following this guideline makes it easier to see the dependencies of a module at a glance.
    \end{quote}

    \item \textbf{API Consistency:} To make the library intuitive, we strive for consistency across the API.
    \begin{itemize}
        \item Parameter names should be consistent with existing functions. For example, use \texttt{cmap} for colormaps, not \texttt{colorscale}.
        \item Functions that handle data should, where possible, accept data directly as a parameter (e.g., a \texttt{y} parameter for an \texttt{NDVar}), analogous to existing plotting functions.
    \end{itemize}
    \begin{quote}
        \textit{Case Study:} During the development of the interactive plotting feature, a reviewer noted that a new function used the parameter name \texttt{colorscale}.
        The contributor was asked to rename it to \texttt{cmap} to match the parameter name used for colormaps throughout the rest of the Eelbrain library.
    \end{quote}

    \item \textbf{Type Hinting:} Use type hints in all function signatures (e.g., \texttt{def my\_function(y: NDVar) -> Figure:}).
    When type hints are present in the signature, they should be omitted from the docstring to avoid redundancy.

    \item \textbf{Docstrings:} All public functions and classes should have clear and informative docstrings.
    Documentation for a class's \texttt{\_\_init\_\_} method should be included in the main class docstring.

    \item \textbf{Data Access:} When working with \texttt{NDVar} objects, use the \texttt{.get\_data()} method to ensure the data axes are in the desired order, as the internal order is not guaranteed.

    \item \textbf{TODOs:} For in-code reminders, use a \texttt{TODO:} tag. For more significant or non-localized tasks, please open a GitHub issue instead.

    \item \textbf{Development Tools:} To streamline the development process and maintain code quality:
    \begin{itemize}
        \item Use \texttt{flake8} locally to check code compliance before submitting
        \item Consider using \texttt{autopep8} to automatically fix common style issues
        \item Configure your IDE to automatically handle formatting (e.g., PyCharm can manage whitespace issues automatically)
        \item Run local checks to catch style problems before they appear in CI
    \end{itemize}

    \item \textbf{API Design Principles:} When designing new functionality:
    \begin{itemize}
        \item Maintain parameter naming consistency across the codebase (e.g., use \texttt{cmap} for colormaps, not \texttt{colorscale})
        \item Follow existing patterns for data input (e.g., accept data directly via a \texttt{y} parameter like other plotting functions)
        \item Consider usability in Jupyter environments during design
        \item Document expected data formats clearly for users
    \end{itemize}
\end{itemize}

\subsection{Architecture and Dependency Guidelines (Learned from LiveNeuron)}
To keep contributions sustainable and reviews effective, follow these project-level guidelines:
\begin{itemize}
    \item \textbf{Reduce complexity via modular boundaries}: keep the Eelbrain core unchanged when possible; place specialized visualization or UI features in separate packages.
    \item \textbf{Isolate dependencies}: avoid mixing incompatible stacks (e.g., Plotly vs. Matplotlib) within one module; separate packages prevent CI failures and version conflicts.
    \item \textbf{Prefer smaller, focused units}: submit smaller repositories/modules and small PRs to simplify review, testing, and independent evolution.
\end{itemize}

\subsection{The Pull Request (PR) Workflow}
We use a pull request-based workflow for all contributions.
\begin{enumerate}
    \item \textbf{Create a branch:} Create a new branch from \texttt{main} for your feature or bugfix.
    \item \textbf{Make your changes:} Make your code changes, ensuring you follow the coding conventions.
    \item \textbf{Submit a Pull Request:} When your changes are ready, push your branch to your fork and open a pull request against the \texttt{main} branch of the official Eelbrain repository.
    \begin{itemize}
        \item Provide a clear and descriptive title for your PR.
        \item In the description, explain the purpose of your changes and link to any relevant GitHub issues.
    \end{itemize}
    \item \textbf{Keep PRs small:} Whenever possible, break down large features into smaller, logically distinct pull requests. Small PRs are much easier and faster to review.
    \begin{quote}
        \textit{Case Study:} The initial implementation of the ``Live Neuron'' feature was submitted as a single large PR (+1,873 lines of code).
        Reviewers noted that this made the review process very challenging.
        This experience highlighted the importance of smaller, more focused PRs to facilitate timely and effective feedback.
    \end{quote}
    \item \textbf{Address feedback:} Engage with the code review process by responding to comments and pushing new commits to your branch to address the feedback.
\end{enumerate}

\subsection{Testing and Validation}
Eelbrain relies on a robust test suite to ensure the correctness and reliability of its scientific algorithms.
\begin{itemize}
    \item \textbf{Automated Testing:} All pull requests trigger a Continuous Integration (CI) workflow that automatically runs the full test suite.
    Please ensure all tests are passing before requesting a review.
    \begin{quote}
        \textit{Case Study:} The large ``Live Neuron'' PR initially failed the automated tests because a new dependency (Plotly) created conflicts with the existing Matplotlib-based code.
        This demonstrated the value of CI in catching integration issues early.
        The eventual solution was to develop the new feature as a separate, independent module to resolve the conflict.
    \end{quote}

    \item \textbf{Writing New Tests:} Any new feature or bugfix should be accompanied by corresponding unit tests.
    This helps prevent future regressions and validates that your code is working as expected.

    \item \textbf{Integration Testing:} For new modules (especially those with external dependencies like Plotly), ensure they:
    \begin{itemize}
        \item Are compatible with the Jupyter notebook environment
        \item Do not conflict with existing dependencies (particularly Matplotlib)
        \item Work across supported platforms (Linux, Windows 10+, macOS)
        \item Meet scientific visualization quality standards for accuracy and reproducibility
    \end{itemize}

    \item \textbf{Performance Considerations:} When implementing interactive features:
    \begin{itemize}
        \item Test with realistic data sizes and ensure reasonable response times
        \item Consider memory usage for large datasets
        \item Optimize for common use cases (e.g., showing important dipoles rather than all dipoles)
        \item Verify that plot sizing works correctly within Jupyter cell outputs
    \end{itemize}
\end{itemize}

\subsection{Code Review Process}
The code review process is a collaborative effort to improve the quality of the codebase.
\begin{itemize}
    \item \textbf{What to Expect:} A core developer will review your pull request and may provide feedback on various aspects, including correctness, adherence to coding standards, API design, and usability.
    The process is iterative; you'll be expected to update your PR based on the feedback.

    \item \textbf{Applying Feedback:} When you receive feedback (e.g., about import order or type hints), please check your entire contribution to see if the same feedback applies elsewhere.
    This helps streamline the review process.
    \begin{quote}
        \textit{Case Study:} A reviewer provided feedback on the import order in one file. Later in the same review, the same issue was found in another file.
        This led to a friendly reminder to contributors to apply feedback globally across their entire PR, which makes the review more efficient for everyone.
    \end{quote}

    \item \textbf{Goal:} The goal of the review is not just to find errors but to refine the design and ensure the new code is well-integrated into the existing project.
    Once the core technical aspects are settled, we may also seek feedback from other users on the usability of a new feature.

    \item \textbf{Common Review Focus Areas:} Based on actual code review experiences, reviewers typically examine:
    \begin{itemize}
        \item API consistency with existing functions (parameter names, data input patterns)
        \item Code organization and import structure
        \item Documentation completeness and clarity (including links to relevant external documentation)
        \item Error handling for edge cases (e.g., data without certain dimensions)
        \item User experience considerations (layout compactness, interaction patterns)
        \item Performance implications for large datasets
    \end{itemize}

    \item \textbf{Response to Feedback:} When addressing reviewer comments:
    \begin{itemize}
        \item Apply feedback comprehensively across your entire contribution
        \item Ask clarifying questions if requirements are unclear
        \item Consider creating follow-up PRs for related improvements suggested during review
        \item Document any design decisions that may not be immediately obvious
    \end{itemize}
\end{itemize}

\subsection{Scientific Software Requirements}
\label{sec:scientific_requirements}
As a scientific computing library, Eelbrain has additional requirements beyond typical software projects:

\begin{itemize}
    \item \textbf{Reproducibility:} All visualizations and computational results must be reproducible across different platforms and Python environments.
    Code should produce consistent results given the same input data and parameters.

    \item \textbf{Scientific Accuracy:} Visualizations must accurately represent the underlying neuroscience data without introducing artifacts or misleading interpretations.
    This includes proper handling of coordinate systems, color scales, and data transformations.

    \item \textbf{Domain Compatibility:} New features should integrate seamlessly with the neuroscience workflow:
    \begin{itemize}
        \item Support standard data formats (NDVar objects with appropriate dimensions)
        \item Work within Jupyter notebook environments commonly used by researchers
        \item Provide clear documentation of data expectations and output formats
        \item Consider the typical use cases of neuroscience researchers
    \end{itemize}

    \item \textbf{Performance for Research Data:} Neuroscience datasets can be large and complex:
    \begin{itemize}
        \item Test with realistic data sizes (multiple subjects, high temporal resolution)
        \item Optimize for common operations (time series visualization, source localization displays)
        \item Provide user control over performance trade-offs (e.g., showing all vs. significant dipoles)
    \end{itemize}
\end{itemize}

By investing in such a guide, the Eelbrain project can significantly improve its collaborative development process, making it more efficient, inclusive, and sustainable.
